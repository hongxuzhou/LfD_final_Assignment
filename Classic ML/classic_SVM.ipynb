{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic ML -- SVM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data analysis libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# For data preprocessing and cleaning\n",
    "import emoji \n",
    "from emoji import demojize, emojize\n",
    "\n",
    "# NLP Libraries\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords # I dont want to use this one for now\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# For text preprocessing\n",
    "import string\n",
    "import contractions\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the tsv file\n",
    "def load_data(train_path, dev_path, text_path):\n",
    "    train_df = pd.read_csv('/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/train.tsv', sep='\\t')\n",
    "    dev_df = pd.read_csv('/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/dev.tsv', sep='\\t')\n",
    "    test_df = pd.read_csv('/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/test.tsv', sep = '\\t') \n",
    "\n",
    "    # Split the data into features and labels\n",
    "    X_train = train_df.iloc[:, 0] # The first column is the tweets\n",
    "    y_train = train_df.iloc[:, 1] # The second column is the labels\n",
    "\n",
    "    # Follow the same steps for the dev set\n",
    "    X_dev = dev_df.iloc[:, 0]\n",
    "    y_dev = dev_df.iloc[:, 1]\n",
    "\n",
    "    # Follow the same steps for the test set\n",
    "    X_test = test_df.iloc[:, 0]\n",
    "    y_test = test_df.iloc[:, 1]\n",
    "\n",
    "    # Map the labels to integers\n",
    "    label_map = {'OFF': 1, 'NOT': 0} # is this step necessary?\n",
    "    y_train = y_train.map(label_map)\n",
    "    y_dev = y_dev.map(label_map)\n",
    "    y_test = y_test.map(label_map)\n",
    "\n",
    "    return (X_train, y_train), (X_dev, y_dev), (X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the labels fro classification report\n",
    "class_names = ['Offensive', 'Not Offensive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pre-processing  \n",
    "1. URL & User Handling\n",
    "2. Emoji Handling\n",
    "3. Text standardisation\n",
    "4. Class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Class to handle text preprocessing for Twitter data\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Common patterns\n",
    "        self.url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "        self.user_pattern = r'@[\\w\\d_]+'\n",
    "        \n",
    "        # Words with 3 or more repeated characters\n",
    "        self.repeat_pattern = re.compile(r'(.)\\1{2,}')\n",
    "        \n",
    "        # Optional: Initialize spacy if needed\n",
    "        # self.nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def standardize_special_tokens(self, text: str) -> str:\n",
    "        \"\"\"Standardize URLs and User mentions\"\"\"\n",
    "        # Replace URLs with [URL]\n",
    "        text = re.sub(self.url_pattern, '[URL]', text)\n",
    "        # Replace @mentions with [USER]\n",
    "        text = re.sub(self.user_pattern, '[USER]', text)\n",
    "        return text\n",
    "    \n",
    "    def handle_emojis(self, text: str) -> str:\n",
    "        \"\"\"Convert emojis to text description\"\"\"\n",
    "        try:\n",
    "            return emoji.demojize(text, delimiters=(' [EMOJI_', '] '))\n",
    "        except:\n",
    "            # If emoji conversion fails, return original text\n",
    "            return text\n",
    "    \n",
    "    def standardize_text(self, text: str) -> str:\n",
    "        \"\"\"Handle non-standard text features\"\"\"\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Expand contractions\n",
    "        try:\n",
    "            text = contractions.fix(text)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Handle repeated characters (e.g., 'sooooo' -> 'soo')\n",
    "        text = self.repeat_pattern.sub(r'\\1\\1', text)\n",
    "        \n",
    "        # Replace multiple spaces with single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def preprocess(self, text: str, \n",
    "                  handle_emojis: bool = False, \n",
    "                  standardize_tokens: bool = False,\n",
    "                  standardize_text: bool = True) -> str:\n",
    "        \"\"\"Main preprocessing function\"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            return ''\n",
    "            \n",
    "        if standardize_tokens:\n",
    "            text = self.standardize_special_tokens(text)\n",
    "            \n",
    "        if handle_emojis:\n",
    "            text = self.handle_emojis(text)\n",
    "            \n",
    "        if standardize_text:\n",
    "            text = self.standardize_text(text)\n",
    "            \n",
    "        return text.strip()\n",
    "\n",
    "def create_preprocessing_pipeline():\n",
    "    \"\"\"Create preprocessing pipeline to be used in CountVectorizer\"\"\"\n",
    "    preprocessor = TextPreprocessor()\n",
    "    return preprocessor.preprocess\n",
    "\n",
    "# Add this function to help analyze preprocessing results\n",
    "def analyze_preprocessing(preprocessor: TextPreprocessor, texts: List[str], n_samples: int = 5):\n",
    "    \"\"\"Analyze the effect of preprocessing on sample texts\"\"\"\n",
    "    print(\"Preprocessing Analysis:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for i, text in enumerate(texts[:n_samples]):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(\"Original:\", text)\n",
    "        print(\"Preprocessed:\", preprocessor.preprocess(text))\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Pre-processing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. Load the data first\n",
    "    (X_train, y_train), (X_dev, y_dev), (X_test, y_test) = load_data(\n",
    "        '/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/train.tsv', \n",
    "        '/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/dev.tsv', \n",
    "        '/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/test.tsv'\n",
    "    )\n",
    "\n",
    "    # 2. Create preprocessor and analyze samples\n",
    "    preprocessor = TextPreprocessor()\n",
    "    sample_texts = X_train.iloc[:5].tolist()\n",
    "    analyze_preprocessing(preprocessor, sample_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basic_svm(max_features=10000, ngram_range=(1, 1)):\n",
    "    \"\"\"\n",
    "    Create a basic SVM model using the CountVectorizer and LinearSVC\n",
    "    \"\"\"\n",
    "    # Create a pipeline\n",
    "    pipeline = Pipeline([\n",
    "        # Will add feature extraction and classification steps here\n",
    "        ('vectorizer', CountVectorizer(\n",
    "            lowercase=False, # Completed this step in the preprocessor\n",
    "            max_features=max_features, # May need to change it for testing \n",
    "            ngram_range=ngram_range, # Consider incleasing it for testing\n",
    "            strip_accents='unicode' # Can it manage emojis?\n",
    "            )),\n",
    "        # Dont forget to try tf-idf\n",
    "        ('classifier', LinearSVC(C=0.1, # Decrease the weight of the majority class to reduce overfitting\n",
    "                                 class_weight='balanced', # Decrease the weight of the majority class to reduce overfitting\n",
    "                                 dual=False, # Set to False when n_samples > n_features\n",
    "                                 ))\n",
    "    ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "def train_model(pipeline, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "    \"\"\"\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train & Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Evaluate and result print function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalute_model(pipeline, X, y, split_name=\"\"): # Why?\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    # Get predictions\n",
    "    y_pred = pipeline.predict(X)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y, y_pred)\n",
    "    class_report = classification_report(y, y_pred, output_dict= True, target_names=class_names)\n",
    "    conf_matrix = confusion_matrix(y, y_pred)\n",
    "    \n",
    "    # Print results \n",
    "    print(f\"\\nResults for {split_name} set:\")\n",
    "    print(f'Accuracy: {acc: .4f}')\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cbar=False, cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix for {split_name} set')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Classification Report\": class_report,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Predictions': y_pred\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Main execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # 1. Load the data\n",
    "    (X_train, y_train), (X_dev, y_dev), (X_test, y_test) = load_data('/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/train.tsv', \n",
    "                                                                     '/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/dev.tsv', \n",
    "                                                                     '/Users/hongxuzhou/LfD/LfD_final_Assignment/data_sets/test.tsv'\n",
    "                                                                     )\n",
    "    \n",
    "    # 2. Create and train the pipeline \n",
    "    pipeline = create_basic_svm(\n",
    "        max_features=100000, # May need to change it for testing\n",
    "        ngram_range=(1, 2) # change uni-gram to bi-gram\n",
    "    )\n",
    "    \n",
    "    pipeline = train_model(pipeline, X_train, y_train)\n",
    "    \n",
    "    # 3. Evaluate the model\n",
    "    train_results = evalute_model(pipeline, X_train, y_train, \"Train\")\n",
    "    dev_results = evalute_model(pipeline, X_dev, y_dev, \"Dev\")\n",
    "    #test_results = evalute_model(pipeline, X_test, y_test, \"Test\") # We dont run it for now\n",
    "    \n",
    "    # 4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
